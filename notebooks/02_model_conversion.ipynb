{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMPo4BcXAzVpbKDli9tCbFA"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Phase 2: Model Conversion"
   ],
   "metadata": {
    "id": "E06fSXYcHF_M"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook converts the custom trained .h5 models into edge-optimized formats.\n",
    "\n",
    "*   TensorFlow.js format (.json + .bin) for use in a Progressive Web App (PWA)\n",
    "*   TensorFlow Lite format (.tflite) for use in mobile apps\n",
    "\n",
    "This ensures models are ready for execution on-device, enabling real-time predictions without server calls.\n"
   ],
   "metadata": {
    "id": "LHSF5NmIGxRg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Setting up the environment**"
   ],
   "metadata": {
    "id": "0eSUIhU_DFUj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install tensorflowjs==4.5.0 keras==2.15.0 tensorflow==2.15.0 --no-deps\n",
    "!pip install tensorflow-decision-forests==1.8.1\n",
    "!pip install jax==0.4.20 jaxlib==0.4.20"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "B_NeggL6eItD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754278462493,
     "user_tz": -60,
     "elapsed": 109054,
     "user": {
      "displayName": "L L Berkley",
      "userId": "09256118485292110051"
     }
    },
    "outputId": "dd61f2a5-26f6-4a3f-b6b0-19251562a190",
    "collapsed": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(\"✅ NumPy:\", np.__version__)\n",
    "print(\"✅ TensorFlow:\", tf.__version__)\n",
    "print(\"✅ Keras:\", keras.__version__)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xiDNpxUcJlmM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754278558528,
     "user_tz": -60,
     "elapsed": 4816,
     "user": {
      "displayName": "L L Berkley",
      "userId": "09256118485292110051"
     }
    },
    "outputId": "a9008cd1-3ab0-4a3b-bedc-ecfc47acfc42"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ NumPy: 1.26.4\n",
      "✅ TensorFlow: 2.15.0\n",
      "✅ Keras: 2.15.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import json\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "id": "l2qnh0DKDIVt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754278580846,
     "user_tz": -60,
     "elapsed": 341,
     "user": {
      "displayName": "L L Berkley",
      "userId": "09256118485292110051"
     }
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# define paths\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Base paths\n",
    "BASE_PATH = \"/content/drive/My Drive/Edge_AI_Project/models\"\n",
    "H5_PATH = os.path.join(BASE_PATH, \"saved_models\")\n",
    "TFJS_PATH = os.path.join(BASE_PATH, \"tfjs_models\")\n",
    "TFLITE_PATH = os.path.join(BASE_PATH, \"tflite_models\")\n",
    "LOG_PATH = os.path.join(BASE_PATH, \"conversion_logs\")\n",
    "\n",
    "# Create fresh directories\n",
    "os.makedirs(TFJS_PATH, exist_ok=True)\n",
    "os.makedirs(TFLITE_PATH, exist_ok=True)\n",
    "os.makedirs(LOG_PATH, exist_ok=True)\n",
    "\n",
    "print(\"✅ Directory structure created\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fsJROdhkQD_4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754278611698,
     "user_tz": -60,
     "elapsed": 20057,
     "user": {
      "displayName": "L L Berkley",
      "userId": "09256118485292110051"
     }
    },
    "outputId": "c85e4c3b-550e-41ba-8048-d977fc5e5e47"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "✅ Directory structure created\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Model configuration**"
   ],
   "metadata": {
    "id": "5ieil97eKV8l"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "MODELS_CONFIG = {\n",
    "    \"age\": {\n",
    "        \"h5_filename\": \"age_model.h5\",\n",
    "        \"input_shape\": (224, 224, 3),\n",
    "        \"description\": \"Adult vs Elderly classification\"\n",
    "    },\n",
    "    \"gender\": {\n",
    "        \"h5_filename\": \"gender_model.h5\",\n",
    "        \"input_shape\": (224, 224, 3),\n",
    "        \"description\": \"Male vs Female classification\"\n",
    "    },\n",
    "    \"emotion\": {\n",
    "        \"h5_filename\": \"emotion_model.h5\",\n",
    "        \"input_shape\": (48, 48, 1),\n",
    "        \"description\": \"Happy vs Sad classification\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Model configurations defined\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bz9bINZgKZjz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754278630169,
     "user_tz": -60,
     "elapsed": 50,
     "user": {
      "displayName": "L L Berkley",
      "userId": "09256118485292110051"
     }
    },
    "outputId": "975c4d38-606d-4d63-965f-1f29da520393"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Model configurations defined\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Main conversion function**"
   ],
   "metadata": {
    "id": "gqaMucVXI5E-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def convert_model_native(model_name, config):\n",
    "    \"\"\"\n",
    "    Direct conversion using TensorFlow's native tools\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CONVERTING {model_name.upper()} MODEL (NATIVE APPROACH)\")\n",
    "    print(f\"{config['description']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    h5_path = os.path.join(H5_PATH, config['h5_filename'])\n",
    "    tfjs_dir = os.path.join(TFJS_PATH, model_name)\n",
    "    tflite_path = os.path.join(TFLITE_PATH, f\"{model_name}.tflite\")\n",
    "\n",
    "    # Check if original model exists\n",
    "    if not os.path.exists(h5_path):\n",
    "        print(f\"❌ Original model not found: {h5_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Step 1: Load original model\n",
    "        print(f\" Loading original model...\")\n",
    "        original_model = load_model(h5_path)\n",
    "        original_model = rebuild_model_with_input_shape(original_model, config['input_shape'])\n",
    "        print(f\"   Original: {original_model.input_shape} -> {original_model.output_shape}\")\n",
    "        print(f\"   Layers: {len(original_model.layers)}\")\n",
    "\n",
    "        # Step 2: Test the original model (verify it works)\n",
    "        print(f\"Testing original model...\")\n",
    "        test_input = np.random.random((1,) + config['input_shape']).astype(np.float32)\n",
    "        test_output = original_model.predict(test_input, verbose=0)\n",
    "        print(f\"   Test: {test_input.shape} -> {test_output.shape}\")\n",
    "        print(f\"   Output range: [{test_output.min():.3f}, {test_output.max():.3f}]\")\n",
    "\n",
    "        # Step 3: Convert to TensorFlow.js (preserves all weights)\n",
    "        print(f\"Converting to TensorFlow.js (native)...\")\n",
    "        tfjs_success = convert_to_tensorflowjs_native(original_model, tfjs_dir, model_name)\n",
    "\n",
    "        # Step 4: Convert to TensorFlow Lite (preserves all weights)\n",
    "        print(f\"Converting to TensorFlow Lite (native)...\")\n",
    "        tflite_success = convert_to_tflite_native(original_model, tflite_path, model_name)\n",
    "\n",
    "        # Compile results\n",
    "        results = {\n",
    "            'model': model_name,\n",
    "            'description': config['description'],\n",
    "            'input_shape': config['input_shape'],\n",
    "            'original_layers': len(original_model.layers),\n",
    "            'clean_layers': len(original_model.layers),  # Same as original\n",
    "            'weights_transferred': True,  # using original\n",
    "            'tfjs_success': tfjs_success,\n",
    "            'tflite_success': tflite_success,\n",
    "            'tfjs_path': tfjs_dir if tfjs_success else None,\n",
    "            'tflite_path': tflite_path if tflite_success else None\n",
    "        }\n",
    "\n",
    "        if tfjs_success and tflite_success:\n",
    "            print(f\"✅ {model_name.upper()} CONVERSION COMPLETED SUCCESSFULLY\")\n",
    "        else:\n",
    "            print(f\"⚠️  {model_name.upper()} CONVERSION COMPLETED WITH ISSUES\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ CONVERSION FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ],
   "metadata": {
    "id": "fdKe7zyqQbG8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754278633204,
     "user_tz": -60,
     "elapsed": 5,
     "user": {
      "displayName": "L L Berkley",
      "userId": "09256118485292110051"
     }
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Rebuild model with input shape**"
   ],
   "metadata": {
    "id": "WQ_89_wQZLe2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def rebuild_model_with_input_shape(model, input_shape):\n",
    "    \"\"\"\n",
    "    Wraps the existing model with an Input layer to explicitly define input shape.\n",
    "    This ensures TF.js conversion doesn't fail due to missing batchInputShape.\n",
    "    \"\"\"\n",
    "    print(f\"🔧 Rebuilding model with explicit Input(shape={input_shape})\")\n",
    "    new_input = Input(shape=input_shape)\n",
    "    new_output = model(new_input)\n",
    "    new_model = Model(inputs=new_input, outputs=new_output)\n",
    "    return new_model\n"
   ],
   "metadata": {
    "id": "r9HbyIyoZJp6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754278639908,
     "user_tz": -60,
     "elapsed": 6,
     "user": {
      "displayName": "L L Berkley",
      "userId": "09256118485292110051"
     }
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. TensorFlow.js conversion function**"
   ],
   "metadata": {
    "id": "UmypjqNXJLYX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def convert_to_tensorflowjs_native(model, output_dir, model_name):\n",
    "    \"\"\"\n",
    "    Convert original Keras model directly to TensorFlow.js using native tools\n",
    "    \"\"\"\n",
    "    print(f\"   🔄 Converting {model_name} to TensorFlow.js (native)...\")\n",
    "\n",
    "    try:\n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Direct conversion using tensorflowjs - this preserves the exact architecture\n",
    "        tfjs.converters.save_keras_model(\n",
    "            model,\n",
    "            output_dir\n",
    "        )\n",
    "\n",
    "        # Check file sizes and verify conversion success\n",
    "        model_json_path = os.path.join(output_dir, 'model.json')\n",
    "        if os.path.exists(model_json_path):\n",
    "            json_size = os.path.getsize(model_json_path) / 1024\n",
    "            print(f\"   📊 Model JSON: {json_size:.1f} KB\")\n",
    "\n",
    "            # Check for weight files (.bin files)\n",
    "            weight_files = [f for f in os.listdir(output_dir) if f.endswith('.bin')]\n",
    "            total_weight_size = sum(os.path.getsize(os.path.join(output_dir, f)) for f in weight_files)\n",
    "            print(f\"   📊 Model weights: {total_weight_size/1024:.1f} KB ({len(weight_files)} files)\")\n",
    "\n",
    "            print(f\"   ✅ TensorFlow.js conversion successful\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"   ❌ TensorFlow.js conversion failed: model.json not found\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ TensorFlow.js conversion failed: {e}\")\n",
    "        return False"
   ],
   "metadata": {
    "id": "NLu9WB0hJJLS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754278642931,
     "user_tz": -60,
     "elapsed": 16,
     "user": {
      "displayName": "L L Berkley",
      "userId": "09256118485292110051"
     }
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. TensorFlow Lite conversion function**"
   ],
   "metadata": {
    "id": "wvoBAnwwJWEK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def convert_to_tflite_native(model, output_path, model_name):\n",
    "    \"\"\"\n",
    "    Convert original Keras model directly to TensorFlow Lite using native tools\n",
    "    \"\"\"\n",
    "    print(f\"   🔄 Converting {model_name} to TensorFlow Lite (native)...\")\n",
    "\n",
    "    try:\n",
    "        # Create TFLite converter from the original Keras model (preserves architecture)\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "        # Apply basic optimizations for edge deployment\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "        # Set quantization settings for better compatibility and smaller size\n",
    "        converter.target_spec.supported_types = [tf.float16]  # Use float16 for smaller size\n",
    "\n",
    "        # Convert the model\n",
    "        tflite_model = converter.convert()\n",
    "\n",
    "        # Save the model\n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "\n",
    "        # Check file size\n",
    "        file_size = os.path.getsize(output_path) / 1024\n",
    "        print(f\"   📊 TFLite model: {file_size:.1f} KB\")\n",
    "        print(f\"   ✅ TensorFlow Lite conversion successful\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ TensorFlow Lite conversion failed: {e}\")\n",
    "        return False\n"
   ],
   "metadata": {
    "id": "w9lcYd3F7_lQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754278646245,
     "user_tz": -60,
     "elapsed": 9,
     "user": {
      "displayName": "L L Berkley",
      "userId": "09256118485292110051"
     }
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**5. Execution function**"
   ],
   "metadata": {
    "id": "T7fxKJS4Jagt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def run_native_conversion():\n",
    "    \"\"\"\n",
    "    Execute native conversion for all models\n",
    "    \"\"\"\n",
    "    print(f\"\\nSTARTING NATIVE MODEL CONVERSION PROCESS\")\n",
    "    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Using TensorFlow's native conversion tools\")\n",
    "    print(f\"Preserves original model architectures and trained weights\")\n",
    "    print(f\"=\"*80)\n",
    "\n",
    "    conversion_results = []\n",
    "\n",
    "    for model_name, config in MODELS_CONFIG.items():\n",
    "        result = convert_model_native(model_name, config)\n",
    "        if result:\n",
    "            conversion_results.append(result)\n",
    "\n",
    "    # Generate summary report\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CONVERSION SUMMARY REPORT\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    success_count = sum(1 for r in conversion_results if r['tfjs_success'] and r['tflite_success'])\n",
    "    print(f\"✅ SUCCESS STATISTICS:\")\n",
    "    print(f\"   TensorFlow.js: {sum(1 for r in conversion_results if r['tfjs_success'])}/{len(conversion_results)} models\")\n",
    "    print(f\"   TensorFlow Lite: {sum(1 for r in conversion_results if r['tflite_success'])}/{len(conversion_results)} models\")\n",
    "    print(f\"\")\n",
    "    print(f\"   ALL MODELS SUCCESSFULLY CONVERTED!\")\n",
    "\n",
    "    print(f\"\\n📂 DOWNLOAD LOCATIONS:\")\n",
    "    for result in conversion_results:\n",
    "        if result['tfjs_success']:\n",
    "            print(f\"   {result['model']}: {result['tfjs_path']}\")\n",
    "        if result['tflite_success']:\n",
    "            print(f\"   {result['model']}: {result['tflite_path']}\")\n",
    "\n",
    "    print(f\"\\n✅ NATIVE CONVERSION PROCESS COMPLETED\")\n",
    "    print(f\"=\"*80)\n",
    "\n",
    "    return conversion_results"
   ],
   "metadata": {
    "id": "bziqD4AAJZ4-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754278649440,
     "user_tz": -60,
     "elapsed": 14,
     "user": {
      "displayName": "L L Berkley",
      "userId": "09256118485292110051"
     }
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**6. Execute the conversion**"
   ],
   "metadata": {
    "id": "CazNO7sFLXNe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "conversion_results = run_native_conversion()"
   ],
   "metadata": {
    "collapsed": true,
    "id": "G4vmrDUoLRDC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754278744450,
     "user_tz": -60,
     "elapsed": 91572,
     "user": {
      "displayName": "L L Berkley",
      "userId": "09256118485292110051"
     }
    },
    "outputId": "b943f7da-e9b5-47c2-bd93-b413e91bf282"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "STARTING NATIVE MODEL CONVERSION PROCESS\n",
      "2025-08-04 03:37:33\n",
      "Using TensorFlow's native conversion tools\n",
      "Preserves original model architectures and trained weights\n",
      "================================================================================\n",
      "\n",
      "======================================================================\n",
      "CONVERTING AGE MODEL (NATIVE APPROACH)\n",
      "Adult vs Elderly classification\n",
      "======================================================================\n",
      " Loading original model...\n",
      "🔧 Rebuilding model with explicit Input(shape=(224, 224, 3))\n",
      "   Original: (None, 224, 224, 3) -> (None, 1)\n",
      "   Layers: 2\n",
      "Testing original model...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Test: (1, 224, 224, 3) -> (1, 1)\n",
      "   Output range: [0.628, 0.628]\n",
      "Converting to TensorFlow.js (native)...\n",
      "   🔄 Converting age to TensorFlow.js (native)...\n",
      "   📊 Model JSON: 117.4 KB\n",
      "   📊 Model weights: 8825.3 KB (3 files)\n",
      "   ✅ TensorFlow.js conversion successful\n",
      "Converting to TensorFlow Lite (native)...\n",
      "   🔄 Converting age to TensorFlow Lite (native)...\n",
      "   📊 TFLite model: 4360.2 KB\n",
      "   ✅ TensorFlow Lite conversion successful\n",
      "✅ AGE CONVERSION COMPLETED SUCCESSFULLY\n",
      "\n",
      "======================================================================\n",
      "CONVERTING GENDER MODEL (NATIVE APPROACH)\n",
      "Male vs Female classification\n",
      "======================================================================\n",
      " Loading original model...\n",
      "🔧 Rebuilding model with explicit Input(shape=(224, 224, 3))\n",
      "   Original: (None, 224, 224, 3) -> (None, 1)\n",
      "   Layers: 2\n",
      "Testing original model...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Test: (1, 224, 224, 3) -> (1, 1)\n",
      "   Output range: [0.962, 0.962]\n",
      "Converting to TensorFlow.js (native)...\n",
      "   🔄 Converting gender to TensorFlow.js (native)...\n",
      "   📊 Model JSON: 117.4 KB\n",
      "   📊 Model weights: 8825.3 KB (3 files)\n",
      "   ✅ TensorFlow.js conversion successful\n",
      "Converting to TensorFlow Lite (native)...\n",
      "   🔄 Converting gender to TensorFlow Lite (native)...\n",
      "   📊 TFLite model: 4361.2 KB\n",
      "   ✅ TensorFlow Lite conversion successful\n",
      "✅ GENDER CONVERSION COMPLETED SUCCESSFULLY\n",
      "\n",
      "======================================================================\n",
      "CONVERTING EMOTION MODEL (NATIVE APPROACH)\n",
      "Happy vs Sad classification\n",
      "======================================================================\n",
      " Loading original model...\n",
      "🔧 Rebuilding model with explicit Input(shape=(48, 48, 1))\n",
      "   Original: (None, 48, 48, 1) -> (None, 1)\n",
      "   Layers: 2\n",
      "Testing original model...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Test: (1, 48, 48, 1) -> (1, 1)\n",
      "   Output range: [1.000, 1.000]\n",
      "Converting to TensorFlow.js (native)...\n",
      "   🔄 Converting emotion to TensorFlow.js (native)...\n",
      "   📊 Model JSON: 7.0 KB\n",
      "   📊 Model weights: 8539.5 KB (3 files)\n",
      "   ✅ TensorFlow.js conversion successful\n",
      "Converting to TensorFlow Lite (native)...\n",
      "   🔄 Converting emotion to TensorFlow Lite (native)...\n",
      "   📊 TFLite model: 4275.5 KB\n",
      "   ✅ TensorFlow Lite conversion successful\n",
      "✅ EMOTION CONVERSION COMPLETED SUCCESSFULLY\n",
      "\n",
      "================================================================================\n",
      "CONVERSION SUMMARY REPORT\n",
      "================================================================================\n",
      "✅ SUCCESS STATISTICS:\n",
      "   TensorFlow.js: 3/3 models\n",
      "   TensorFlow Lite: 3/3 models\n",
      "\n",
      "   ALL MODELS SUCCESSFULLY CONVERTED!\n",
      "\n",
      "📂 DOWNLOAD LOCATIONS:\n",
      "   age: /content/drive/My Drive/Edge_AI_Project/models/tfjs_models/age\n",
      "   age: /content/drive/My Drive/Edge_AI_Project/models/tflite_models/age.tflite\n",
      "   gender: /content/drive/My Drive/Edge_AI_Project/models/tfjs_models/gender\n",
      "   gender: /content/drive/My Drive/Edge_AI_Project/models/tflite_models/gender.tflite\n",
      "   emotion: /content/drive/My Drive/Edge_AI_Project/models/tfjs_models/emotion\n",
      "   emotion: /content/drive/My Drive/Edge_AI_Project/models/tflite_models/emotion.tflite\n",
      "\n",
      "✅ NATIVE CONVERSION PROCESS COMPLETED\n",
      "================================================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**7. BatchInputShape validation**"
   ],
   "metadata": {
    "id": "z57MPyuLLKoG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Verifying the model structure\n",
    "\n",
    "def inspect_graph_model(model_name):\n",
    "    json_path = os.path.join(TFJS_PATH, model_name, \"model.json\")\n",
    "\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"❌ model.json for {model_name} not found.\")\n",
    "        return\n",
    "\n",
    "    with open(json_path, \"r\") as f:\n",
    "        model_json = json.load(f)\n",
    "\n",
    "    print(f\"\\n=== {model_name.upper()} MODEL STRUCTURE ===\")\n",
    "    print(f\"Format: {model_json.get('format', 'unknown')}\")\n",
    "    print(f\"GeneratedBy: {model_json.get('generatedBy', 'unknown')}\")\n",
    "\n",
    "    # Check if there are signature definitions (inputs/outputs)\n",
    "    topology = model_json.get(\"modelTopology\", {})\n",
    "    if \"signature\" in topology:\n",
    "        signature = topology[\"signature\"]\n",
    "        print(f\"Signature inputs: {list(signature.get('inputs', {}).keys())}\")\n",
    "        print(f\"Signature outputs: {list(signature.get('outputs', {}).keys())}\")\n",
    "\n",
    "    # Show first few keys of topology to understand structure\n",
    "    print(f\"Topology keys: {list(topology.keys())}\")\n",
    "\n",
    "# Inspect all models\n",
    "inspect_graph_model(\"age\")\n",
    "inspect_graph_model(\"gender\")\n",
    "inspect_graph_model(\"emotion\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NwYSieoENL5V",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754279500379,
     "user_tz": -60,
     "elapsed": 188,
     "user": {
      "displayName": "L L Berkley",
      "userId": "09256118485292110051"
     }
    },
    "outputId": "b0316c0f-0f51-4e60-850a-5c0db8cdcadc"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=== AGE MODEL STRUCTURE ===\n",
      "Format: layers-model\n",
      "GeneratedBy: keras v2.15.0\n",
      "Topology keys: ['keras_version', 'backend', 'model_config']\n",
      "\n",
      "=== GENDER MODEL STRUCTURE ===\n",
      "Format: layers-model\n",
      "GeneratedBy: keras v2.15.0\n",
      "Topology keys: ['keras_version', 'backend', 'model_config']\n",
      "\n",
      "=== EMOTION MODEL STRUCTURE ===\n",
      "Format: layers-model\n",
      "GeneratedBy: keras v2.15.0\n",
      "Topology keys: ['keras_version', 'backend', 'model_config']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# confirm batch_input_shape is present\n",
    "import os\n",
    "import json\n",
    "\n",
    "def verify_layers_model_input_shape(model_name):\n",
    "    json_path = os.path.join(TFJS_PATH, model_name, \"model.json\")\n",
    "\n",
    "    with open(json_path, \"r\") as f:\n",
    "        model_json = json.load(f)\n",
    "\n",
    "    try:\n",
    "        layers = model_json[\"modelTopology\"][\"model_config\"][\"config\"][\"layers\"]\n",
    "        input_layer = next(l for l in layers if l[\"class_name\"] == \"InputLayer\")\n",
    "        shape = input_layer[\"config\"][\"batch_input_shape\"]\n",
    "        print(f\"✅ {model_name}: batch_input_shape = {shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not extract batch_input_shape for {model_name}: {e}\")\n",
    "\n",
    "verify_layers_model_input_shape(\"age\")\n",
    "verify_layers_model_input_shape(\"gender\")\n",
    "verify_layers_model_input_shape(\"emotion\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ryMvK8mLKOx",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1754279659296,
     "user_tz": -60,
     "elapsed": 103,
     "user": {
      "displayName": "L L Berkley",
      "userId": "09256118485292110051"
     }
    },
    "outputId": "aa2b5761-a2ad-41f7-bba4-864551748b84"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ age: batch_input_shape = [None, 224, 224, 3]\n",
      "✅ gender: batch_input_shape = [None, 224, 224, 3]\n",
      "✅ emotion: batch_input_shape = [None, 48, 48, 1]\n"
     ]
    }
   ]
  }
 ]
}
